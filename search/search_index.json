{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Proposal","text":"<p>Zhiping Liao (zhiping2@andrew.cmu.edu), Siyuan Lin (slin3@andrew.cmu.edu)</p>"},{"location":"#summary","title":"Summary","text":"<p>We are going to implement efficient hash join operators using NVIDIA CUDA. We will focus on OLAP workloads and evaluate select-project-join queries on TPC-H benchmarks by taking advantage of GPU architecture and parallelism.</p>"},{"location":"#background","title":"Background","text":"<p>Join operators are one of the most common physical operators in database management system. In the meantime, they are also one of the most expensive operators, attracting great interests to make them efficient. A join operator typically combines rows from two or more tables with given predicates fetching rows conditionally. It allows efficient data retrieval and problem-solving in complex business queries.</p> <p>Most joins are equi-joins, which use only equality comparisons in the join-predicate [4]. Hash joins are the most important equi-join physical implemenations in the analytical world and are one of the most well-studied among all join algorithms. It motivates us to focus on implementing effcient hash join by taking advantage of GPU architecture. </p> <p>Hash join typically involves two steps: build phase and probe phase.</p> <ul> <li>Build phase will construct the hash table by inserting keys from the build table.</li> <li>Probe phase will iterate the probe table and try to find a match in the hash table, if there is a match, the emit the tuple.</li> </ul> <p>While join is extensively studied under CPU context in mainstream databases, GPU with its massive parallelism and high memory bandwidth, offers great potential for speedup for join operator. GPU with SIMT execution model could fire hundreds of thousands of threads  simultaneously. Thus, both build and probe phase in hash join could be implemented by utilizing parallel threads to construct the hash table and probe the hash table with correct synchronizations.</p>"},{"location":"#the-challenge","title":"The Challenge","text":""},{"location":"#challenge-1","title":"Challenge 1","text":"<p>The first challenge would be to have correct and efficient implementations of GPU hash table.</p> <p>During the build phase, CUDA threads are concurrently inserting entries into a hash table. The table should implement correct synchronizations. Here, atmoic operations are encouraged due to less overhead.</p> <p>Moreover, since keys are randomly distributed in the hash table, how to efficiently exploit GPU memory hierarchy also poses a challenge, as global memory is far slower than cache or shared memory.</p>"},{"location":"#challenge-2","title":"Challenge 2","text":"<p>The second challenge would be to ensure efficient work assignments and fair load balance for each thread.</p> <p>During the probing phase, each thread would need to probe a different key into the hash table, if there is a match, then the thread would emit the tuple into output. It will lead to high thread divergence as in the same warp some threads would hit while others would miss. Moreover, coalesced memory access would become difficult as keys are randomly distributed inside the hash table, and adjacent threads would very unlikely access non-adjacent memory.</p>"},{"location":"#challenge-3","title":"Challenge 3","text":"<p>The third challenge would be scaling to a large dataset.</p> <p>GPU memory is limited. Even on a cutting-edge H100, its memory would be 80GB. It is big but would not be enough when handling real-world dataset where tables could be petabytes large. In this way, it requires GPU out-of-core execution to multiple iterations to move dataset into main memory, process, and store results back. Under this scenario, how to efficiently divide dataset into chunk and how to reduce data transfers would be great challenge as data movement between CPU and GPU would be very slow compared to GPU memory bandwidth (450GB/s vs. 3TB/s on H100 NVLink).</p>"},{"location":"#resources","title":"Resources","text":"<p>We will start codebase with libraries for GPU data structure and algorithms provided by NVIDIA [1, 2], such as hash table implementations in cuCollections, and join algorithms in cuDF. We also want to incorporate an SQL frontend (Apache Calcite, DuckDB, PostgreSQL) which transforms SQL queries into physical plans.</p> <p>From our side, we will implement data loader into the GPU memory and GPU kernels for join evaluation from scratch. Customized kernels offer a more degree of freedom and enable further optimizations. Besides, we will consult to a previous overview [3] of GPU accelerated database, looking for hints and insights on GPU database execution.</p> <p>The development compute resources would be GHC machines with RTX 2080, and the benchmark compute resources would be PSC machines with V100. More advanced machines have higher memory bandwidth and interconnect bandwidth, which allows for more space for optimizations.</p>"},{"location":"#goals-and-deliverables","title":"Goals and Deliverables","text":"<ol> <li>Build a simple but sufficient testing framework that 1) can parse and convert the queries into physical plans, 2) load the data to join into GPUs and 3) execute and evaluate the hash join implementation. We will incorporate open source libraries to streamline it.</li> <li>Implement a baseline inner hash join implementation leveraging cuCollections and cuDF. Complete an end-to-end testing of our framework and make sure things work.</li> <li>Refine inner joins with customized table building, probing strategies, materialized output or other improvements. This will be the major part of our project. We will profile the performance after applying each technique, reason about its fitness in analytical workloads, explore potential improvements and narrow down to a set of optimizations with the best performance.</li> <li>Bonus: implement outter joins to support more usecases.</li> <li>Bonus: implement out-of-core execution for hash join when tables are larger than GPU main memory.</li> </ol>"},{"location":"#platform-choice","title":"Platform Choice","text":"<p>We will choose Linux with NVIDIA GPUs with modern CUDA support as our platforms. The core implementation of the join algorithms will be written in CUDA C++. We would utilize GHC machines (equipped with RTX 2080) as our development environment, and consider PSC (V100) / AWS instances to be our benchmark environment.</p>"},{"location":"#schedule","title":"Schedule","text":"Date Task Mar 27 - April 2 Build the framework supporting query parsing, data loading and benchmarking. April 3 - April 9 Implement and benchmark the baseline hash join implementation, looking for rooms to improve. April 10 - April 16 Prepare the milestone report. Customize the hash table implementation with at least 2 techniques. Benchmark them and looking for further improvements. April 17 - April 23 Devise and benchmark another 2 techniques. Narrow down the optimizations for the best performance. April 24 - April 28 Write the final report. In the mean time, assess the attainablity of a bonus point, implement and discuss its performance characteristics."},{"location":"#references","title":"References","text":"<ol> <li>https://developer.nvidia.com/blog/maximizing-performance-with-massively-parallel-hash-maps-on-gpus/</li> <li>https://www.nvidia.com/en-us/on-demand/session/gtcsiliconvalley2019-s9557/</li> <li>https://arxiv.org/abs/2406.13831v1</li> <li>https://en.wikipedia.org/wiki/Join_(SQL)</li> </ol>"},{"location":"final/","title":"Final Report","text":"<p>Final Report</p>"},{"location":"milestone/","title":"Milestone Report","text":"<p>Zhiping Liao (zhiping2), Siyuan Lin (slin3)</p>"},{"location":"milestone/#revised-schedule","title":"Revised Schedule","text":"Date Task Mar 27 - April 2 Looked for necessary open source components to simplify the data loading and query plan generation. Integreted them to a loading framework. Data loading: Siyuan, Query and plan framework: Zhiping April 3 - April 9 Continued on building the framework. Implement and benchmark the baseline CPU hash join implementation, looking for rooms to improve. Basic operators: Siyuan, Framework: Zhiping Liao April 10 - 16 Wrote the milestone report. Implemented and optimized the GPU-based hash join. Report: Zhiping and Siyuan, GPU: Siyuan April 17 - 23 Devise and benchmark another 2 techniques. Narrow down the optimizations for the best performance. Optimizations: Zhiping and Siyuan April 24 - 28 Write the final report. In the mean time, assess the attainablity of implementing a hash aggragtor on GPU. Report: Zhiping and Siyuan"},{"location":"milestone/#current-progress","title":"Current Progress","text":"<p>The progress we have made till the milestone generally involves several important points below:</p> <ul> <li>Prepared TPC-H dataset:<ul> <li>Cleaned data.</li> <li>Transformed into <code>parquet</code> files for faster loading.</li> <li>Loaded into C++ program using Apache Arrow.</li> <li>Selected and rewrote a number of queries to suit our project\u2019s need.</li> </ul> </li> <li>Implemented fundamental database executor from scratch, including <code>TableScan</code>, <code>Filter</code>, <code>Join</code>, <code>Project</code>. Verified the correctness of these operators on CPU.</li> <li>Built database execution pipeline by reading a query plan and translating into corresponding physical operators:<ul> <li>Used DuckDB to generate query plans and table schemas.</li> <li>Used <code>sqlglot</code> to parse SQL expressions in predicates and join conditions.</li> <li>Curated and rewrote queries to limit them into the operators we chose.</li> <li>Bound query plans and expressions to operators with given table schema.</li> </ul> </li> <li>Zoomed in on the join operator and implemented a baseline GPU join operator using cuco <code>static_multimap</code> host API as starting point.</li> <li>Optimized the GPU join operator by switching to <code>static_multiset</code> to reduce extra data materialization costs and better and handy API usage.</li> </ul>"},{"location":"milestone/#revised-goals-and-delieverables","title":"Revised Goals and Delieverables","text":"<p>As we were exploring the datasets and assessing the feasibility, we noticed several issues:</p> <ol> <li>Semi join and outter join plans result in <code>DELIM</code> joins and scans: 3-way join operators and result reuse, which are non-trivial to support in our framework.</li> <li>Aggregations are challenging to implement and optimize because 1) it requires a hash table just as normal joins, 2) only simple aggregation functions (<code>min</code>, <code>sum</code>, <code>avg</code>) can be implemented and need static dispatch to achieve ideal performance (one CUDA kernel per each aggregation function). We thus decide to choose aggregations as our bonus target.</li> <li>Other expressions and operators would add significant amount of work but do not contribute to the goals of our projects. We agreed to drop their supports and continued to emphasis hash joins on GPU.</li> <li><code>libcudf</code> turns out to be closely couple with the cuDF dataframe representation and its complexity is way beyond our imagination. We settled on NVIDIA\u2019s cuCo (cuCollections) as our references to implement hash join algorithms. However, <code>libcudf</code>'s hash join implementation does provide us with some insights for our design.</li> </ol> <p>Based on our observations, we made a few changes to our goals and deliverables:</p> <ol> <li>Hash join algorithms will only rely on cuco, not cuDF, as it provides sufficient functionality, customizability and better performance. </li> <li>Our further operator optimizations will mainly focus on join performance optimization, including but not limited to custom kernel optimizations, reduced data materialization, data movement and kernel execution overlapping.</li> <li>We will also focus on data-loading or pre-processing before join, as data patterns will affect hash table insertion and lookup, which can indirectly affect cache friendliness and hit rate. This is important for performance speedup.</li> <li>We change our bonus points to implement and refine hash aggregation with a fixed set of functions: <code>sum</code>, <code>min</code>, <code>max</code> and <code>avg</code>.</li> <li>We want to reduce the supports for other operators and expressions. </li> </ol>"},{"location":"milestone/#poster-session-plan","title":"Poster Session Plan","text":"<p>Our poster session will likely to demonstrate:</p> <ol> <li>Background information about our project and why it is challenging;</li> <li>The architecture of the project: what tools are used and how they are chained together;</li> <li>The speedup and performance comparison between different versions;</li> <li>A deep analysis of how we progressively diagnose the performance and arrive at the final solution.</li> </ol>"},{"location":"milestone/#preliminary-results","title":"Preliminary Results","text":"<p>Currently as we only focus on join, so the SQL query for development and benchmark only involves one join, and it is sufficient for us to benchmark and improve on its performance. The query we focus on currently is a simplified TPCH Q21, which eliminates aggregation and sub-query, and only focus on join.</p>"},{"location":"milestone/#q21-simplified-query","title":"Q21 Simplified Query","text":"<p>Setup:</p> <ul> <li>TPC-H dataset scale factor: 10.</li> <li>Machine: <code>g4dn.xlarge</code>, NVIDIA T4 Tensor Core, 4 vCPUs, 16 GB Memory.</li> <li>Environment: <code>amazon/Deep Learning Base OSS Nvidia Driver GPU AMI (Ubuntu 22.04)</code></li> <li>Here we use cuco as the hash table library, and it provides host API that can be called on CPU, and devices API that can only be called inside a kernel.</li> </ul> <pre><code>select\n  s_suppkey, l_orderkey\nfrom\n  supplier,\n  lineitem l1,\nwhere\n  s_suppkey = l1.l_suppkey\n  and s_nationkey = 1\n</code></pre> CPU Join GPU Join with multimap GPU Join with multiset Time 4.8s 5.1s 3.5s <p>Analysis:</p> <ul> <li>For scale factor 10, the large table <code>lineitem</code> would contain 6M rows, and would be as large as 5GB.</li> <li><code>static_multimap</code> is worse because it introduces extra data materialization costs due to the API, whenever there is match, it can only output the build table match, but not specifying probing matching. <code>static_multiset</code>  works as we can encode <code>(Key, row idx)</code> as the key for set, and it could give us correct results without extra looking up the probing table. (Some details are omitted here).</li> <li>There are still some extra data materialization costs and data movement that could be saved by careful optimizations. They take almost half of the time due to frequent CPU-GPU data transfer.</li> </ul>"},{"location":"milestone/#primary-concerns","title":"Primary Concerns","text":"<p>We are concerned about the excessive complexity of our framework (data loading, plan building and expression evaluation). It now poses non-neglegible debugging overheads to us.</p>"}]}